"""
Interact Service (T10) â€” Auto-Reply System with RAG + Draft Mode
- RAG Pipeline (in-memory fallback for MVP)
- Reply draft generation via Claude API
- Risk control via keyword matching
- In-memory draft store (replace with DB in production)
- IG comment reply via Graph API
"""

import os
import uuid
import logging
from datetime import datetime, timezone
from typing import TypedDict

import requests

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Risk Control
# ---------------------------------------------------------------------------

NEGATIVE_KEYWORDS = ["è©é¨™", "å‡çš„", "é€€æ¬¾", "æŠ•è¨´", "æ­»", "çˆ›", "åžƒåœ¾", "é¨™", "é»‘å¿ƒ"]


def check_risk(text: str) -> str:
    """Return 'high' if any negative keyword is found in text, else 'low'."""
    for kw in NEGATIVE_KEYWORDS:
        if kw in text:
            return "high"
    return "low"


# ---------------------------------------------------------------------------
# RAG Pipeline â€” MVP: in-memory fallback, replace with Pinecone in production
# ---------------------------------------------------------------------------

# { persona_id: [ { "id": str, "text": str } ] }
_persona_index: dict[str, list[dict]] = {}


def _persona_json_to_chunks(persona_json: dict) -> list[str]:
    """
    Convert a persona JSON dict into text chunks for indexing.
    Handles PersonaCard fields: name, occupation, personality_tags,
    speech_pattern, values, weekly_lifestyle, appearance, etc.
    """
    chunks = []

    if "name" in persona_json:
        chunks.append(f"åç¨±ï¼š{persona_json['name']}")

    if "occupation" in persona_json:
        chunks.append(f"è·æ¥­ï¼š{persona_json['occupation']}")

    if "personality_tags" in persona_json:
        tags = persona_json["personality_tags"]
        if isinstance(tags, list):
            chunks.append(f"å€‹æ€§æ¨™ç±¤ï¼š{', '.join(tags)}")

    if "speech_pattern" in persona_json:
        chunks.append(f"å£ç™– / èªªè©±æ–¹å¼ï¼š{persona_json['speech_pattern']}")

    if "values" in persona_json:
        vals = persona_json["values"]
        if isinstance(vals, list):
            chunks.append(f"åƒ¹å€¼è§€ï¼š{', '.join(vals)}")

    if "weekly_lifestyle" in persona_json:
        chunks.append(f"ç”Ÿæ´»æ–¹å¼ï¼š{persona_json['weekly_lifestyle']}")

    # Flatten appearance sub-fields
    if "appearance" in persona_json and isinstance(persona_json["appearance"], dict):
        app = persona_json["appearance"]
        for key, val in app.items():
            if val:
                chunks.append(f"å¤–è²Œ-{key}ï¼š{val}")

    # Generic fallback: pick up any top-level string/list not already handled
    handled = {"name", "occupation", "personality_tags", "speech_pattern",
               "values", "weekly_lifestyle", "appearance", "id"}
    for k, v in persona_json.items():
        if k in handled:
            continue
        if isinstance(v, str) and v:
            chunks.append(f"{k}ï¼š{v}")
        elif isinstance(v, list):
            chunks.append(f"{k}ï¼š{', '.join(str(i) for i in v)}")

    return chunks


def build_persona_index(persona_id: str, persona_json: dict) -> None:
    """
    Build (or rebuild) the in-memory RAG index for a persona.
    MVP: in-memory fallback, replace with Pinecone in production.
    """
    chunks = _persona_json_to_chunks(persona_json)
    _persona_index[persona_id] = [
        {"id": f"{persona_id}_{i}", "text": chunk}
        for i, chunk in enumerate(chunks)
    ]
    logger.info("Built persona index for persona_id=%s (%d chunks)", persona_id, len(chunks))


def query_persona_context(persona_id: str, query_text: str, top_k: int = 3) -> list[str]:
    """
    Return the top_k most relevant persona text chunks for the query.
    MVP: in-memory fallback with keyword matching â€” replace with Pinecone in production.
    """
    chunks = _persona_index.get(persona_id, [])
    if not chunks:
        return []

    query_lower = query_text.lower()
    query_tokens = set(query_lower.split())

    # Score each chunk by overlap with query tokens
    scored = []
    for chunk in chunks:
        chunk_lower = chunk["text"].lower()
        # Count how many query tokens appear in this chunk
        score = sum(1 for tok in query_tokens if tok in chunk_lower)
        scored.append((score, chunk["text"]))

    scored.sort(key=lambda x: x[0], reverse=True)
    top = [text for _, text in scored[:top_k]]

    # If no matches, just return first top_k chunks
    if all(s == 0 for s, _ in scored[:top_k]):
        top = [c["text"] for c in chunks[:top_k]]

    return top


# ---------------------------------------------------------------------------
# Reply Draft Generation (Claude API)
# ---------------------------------------------------------------------------

def generate_reply_draft(
    persona_id: str,
    comment_text: str,
    commenter_name: str,
    fan_id: str = "",
) -> str:
    """
    Generate a reply draft using Claude that matches the persona's voice.
    Falls back to a safe canned reply if the API call fails.

    fan_id (optional): when provided, fan memory context is injected into
    the prompt so Claude can personalise the reply for returning fans.
    """
    from app.services import fan_memory_service  # avoid circular import

    context_chunks = query_persona_context(persona_id, comment_text, top_k=3)
    persona_context = "\n".join(context_chunks) if context_chunks else "é€™æ˜¯ä¸€å€‹å‹å–„çš„è™›æ“¬ç¶²ç´…äººè¨­ã€‚"

    # Inject fan memory context if available
    fan_context = fan_memory_service.get_fan_context(persona_id, fan_id) if fan_id else ""
    fan_section = f"\nç²‰çµ²è³‡è¨Šï¼š{fan_context}" if fan_context else ""

    prompt = f"""ä½ æ˜¯ä¸€å€‹ AI è™›æ“¬ç¶²ç´…ï¼Œä»¥ä¸‹æ˜¯ä½ çš„äººè¨­è³‡æ–™ï¼š

{persona_context}{fan_section}

æœ‰ä¸€ä½åå«ã€Œ{commenter_name}ã€çš„ç²‰çµ²åœ¨ Instagram ç•™è¨€ï¼š
ã€Œ{comment_text}ã€

è«‹æ ¹æ“šä½ çš„äººè¨­ï¼Œä»¥ç¬¦åˆä½ å€‹æ€§å’Œèªžæ°£çš„æ–¹å¼ï¼Œæ’°å¯«ä¸€å‰‡å›žè¦†çµ¦é€™ä½ç²‰çµ²ã€‚
è¦æ±‚ï¼š
- è¦ªåˆ‡è‡ªç„¶ï¼Œä¸èƒ½å¤ªæ­£å¼æˆ–å¤ªç”Ÿç¡¬
- ä¸è¶…éŽ 150 å­—
- å¯ä»¥åŠ å…¥é©ç•¶çš„ emoji
- è‹¥ç²‰çµ²è³‡è¨Šä¸­æœ‰ usernameï¼Œå¯ä»¥åœ¨å›žè¦†ä¸­å«å‡ºç²‰çµ²åå­—ï¼Œè®“å›žè¦†æ›´å€‹äººåŒ–
- åªè¼¸å‡ºå›žè¦†å…§å®¹æœ¬èº«ï¼Œä¸è¦åŠ ä»»ä½•èªªæ˜Žæˆ–å‰è¨€"""

    anthropic_key = os.getenv("ANTHROPIC_API_KEY", "")
    if not anthropic_key:
        logger.warning("ANTHROPIC_API_KEY not set, returning fallback draft")
        return f"è¬è¬ {commenter_name} çš„ç•™è¨€ï¼ðŸ˜Š å¾ˆé«˜èˆˆè½åˆ°ä½ çš„è²éŸ³ï¼Œæˆ‘å€‘ä¸‹æ¬¡å†èŠï¼"

    try:
        import anthropic
        client = anthropic.Anthropic(api_key=anthropic_key)
        message = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=300,
            messages=[{"role": "user", "content": prompt}],
        )
        draft = message.content[0].text.strip()
        return draft
    except Exception as exc:
        logger.error("Claude API error: %s", exc)
        return f"è¬è¬ {commenter_name} çš„ç•™è¨€ï¼ðŸ˜Š å¾ˆé«˜èˆˆå’Œä½ äº’å‹•ï¼ŒæœŸå¾…ä½ çš„ä¸‹æ¬¡ç•™è¨€ï¼"


# ---------------------------------------------------------------------------
# Draft Store â€” in-memory for MVP
# ---------------------------------------------------------------------------

class ReplyDraft(TypedDict):
    reply_id: str
    persona_id: str
    ig_comment_id: str
    ig_media_id: str
    commenter_name: str
    comment_text: str
    draft_text: str
    risk_level: str   # "high" | "low"
    status: str       # "pending" | "sent" | "dismissed"
    created_at: str   # ISO datetime


# { reply_id: ReplyDraft }
pending_replies: dict[str, ReplyDraft] = {}

# { persona_id: "draft" | "auto" }
_auto_reply_settings: dict[str, str] = {}


def add_pending_reply(
    persona_id: str,
    ig_comment_id: str,
    ig_media_id: str,
    commenter_name: str,
    comment_text: str,
    draft_text: str,
    risk_level: str,
) -> ReplyDraft:
    """Add a new reply draft to the pending store."""
    reply_id = str(uuid.uuid4())
    draft: ReplyDraft = {
        "reply_id": reply_id,
        "persona_id": persona_id,
        "ig_comment_id": ig_comment_id,
        "ig_media_id": ig_media_id,
        "commenter_name": commenter_name,
        "comment_text": comment_text,
        "draft_text": draft_text,
        "risk_level": risk_level,
        "status": "pending",
        "created_at": datetime.now(timezone.utc).isoformat(),
    }
    pending_replies[reply_id] = draft
    logger.info(
        "Added pending reply reply_id=%s persona_id=%s risk=%s",
        reply_id, persona_id, risk_level,
    )
    return draft


def get_pending_replies(persona_id: str) -> list[ReplyDraft]:
    """Return all pending-status drafts for the given persona."""
    return [
        draft for draft in pending_replies.values()
        if draft["persona_id"] == persona_id and draft["status"] == "pending"
    ]


def send_reply(reply_id: str, access_token: str) -> ReplyDraft:
    """
    Post the draft reply to Instagram via the Graph API, then mark as sent.
    IG API: POST https://graph.instagram.com/v18.0/{comment_id}/replies
    """
    draft = pending_replies.get(reply_id)
    if not draft:
        raise ValueError(f"Reply not found: {reply_id}")
    if draft["status"] != "pending":
        raise ValueError(f"Reply is not in pending state: {draft['status']}")

    ig_comment_id = draft["ig_comment_id"]
    message_text = draft["draft_text"]

    # Call IG Graph API to post reply
    url = f"https://graph.instagram.com/v18.0/{ig_comment_id}/replies"
    try:
        resp = requests.post(url, params={
            "message": message_text,
            "access_token": access_token,
        }, timeout=15)
        resp.raise_for_status()
        logger.info("Sent reply for reply_id=%s ig_comment_id=%s", reply_id, ig_comment_id)
    except Exception as exc:
        logger.error("Failed to send reply to IG: %s", exc)
        raise RuntimeError(f"IG API error: {exc}") from exc

    draft["status"] = "sent"
    return draft


def dismiss_reply(reply_id: str) -> ReplyDraft:
    """Mark a reply draft as dismissed."""
    draft = pending_replies.get(reply_id)
    if not draft:
        raise ValueError(f"Reply not found: {reply_id}")
    draft["status"] = "dismissed"
    logger.info("Dismissed reply reply_id=%s", reply_id)
    return draft


def get_auto_reply_setting(persona_id: str) -> str:
    """Return the current auto-reply mode for a persona ('draft' or 'auto')."""
    return _auto_reply_settings.get(persona_id, "draft")


def set_auto_reply_setting(persona_id: str, mode: str) -> None:
    """Set the auto-reply mode for a persona ('draft' or 'auto')."""
    if mode not in ("draft", "auto"):
        raise ValueError(f"Invalid mode: {mode}. Must be 'draft' or 'auto'.")
    _auto_reply_settings[persona_id] = mode
    logger.info("Set auto-reply mode for persona_id=%s â†’ %s", persona_id, mode)
